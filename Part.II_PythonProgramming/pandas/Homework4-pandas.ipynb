{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - pandas (YOUR NAME HERE)\n",
    "### due 03/10/2021, to be turned on github  \n",
    "\n",
    "<br>\n",
    "\n",
    "Use this notebook and prompts to complete the homework. Throughout there will be hints and code provided to\n",
    "\n",
    "### Things you will need:\n",
    "- Install os, NumPy, pandas\n",
    "- states_covid.csv\n",
    "- Bloom_etal_2018_Reduced_Dataset.csv\n",
    "- logfiles.tgz (or some other multiple file dataset)\n",
    "\n",
    "**hint:** change your working directory to a directory for just this homework. Edit path below and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_dir = \"your/homework/path/here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd $hw_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import packages & check required datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(hw_dir,'states_covid.csv')), 'states_covid.csv does not exist' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(hw_dir,'Bloom_etal_2018_Reduced_Dataset.csv')), 'Bloom_etal_2018_Reduced_Dataset.csv does not exist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf logfiles.tgz\n",
    "log_dir = os.path.join(hw_dir,'logfiles')\n",
    "assert os.path.exists(log_dir), 'log_dir does not exist' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - DataFrame manipulation\n",
    "\n",
    "Using **states_covid.csv**, we are going to read the data in as a DataFrame to manipulate, subset, and filter in various ways. \n",
    "\n",
    "**1.1 Read in states_covid.csv with date as a \"date\" dtype, and only columns consisting of the hospitalization (4 col), ICU (2 col), and Ventilators (2 col)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 List, in order, the 5 states with the highest numbers of people *currently* hospitalized, in the ICU, and on ventilation**  \n",
    "hint: sort_values, uniqie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Identify the date for each state where the cumulative hosipitalized was greater than or equal to 1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - DataFrame summarizing\n",
    "\n",
    "Using **Bloom_etal_2018_Reduced_Dataset.csv**, we are going to do more dataframe manipulation and subsetting and summarizing  \n",
    "\n",
    "**2.1 Read in Bloom_etal_2018_Reduced_Dataset.csv and create two new columns ('genus','species') that consists of the column *taxa* split at the underscore. Print out the head of this new dataframe and the number of unique genera**   \n",
    "\n",
    "*hint:* pd.str.split(,expand=True)\n",
    "\n",
    "for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| taxa | genus | species \n",
    "| :------ | :-- | :--- \n",
    "| Alosa_alabamae | Alosa | alabamae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Create a new dataframe with the mean *logbodysize* and *trophicposition* of each genera. Sort this data frame by the largest body size. Print the head of this dataframe. Which genera is the smallest and largest? What is the trophic position of the smallest and largest?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Read in muliple files to a dictionary and make a DataFrame\n",
    "\n",
    "### This is not an easy excersize. You will have some example code to start but the rest is up to you. You have done all these parts before so should just be linking them together\n",
    "\n",
    "Using **logfiles**: we are going to do read in each file, get some data, append it to a dictionary to later make into a dataframe. \n",
    "\n",
    "First step is to find the necessary files. The number of files in the log files is 36, make sure you have that many as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l logfiles/*txt | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfiles = !find ./logfiles -name *txt #unix command to find files\n",
    "logfiles = [os.path.abspath(x) for x in logfiles] #this finds the full path to the file\n",
    "print(logfiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(logfiles)==36), 'Do not have correct number of logfiles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a little tricky here\n",
    "\n",
    "Read in each of the logfiles, for each file extract:\n",
    "- minimum temperature\n",
    "- maximum temperature\n",
    "- date of minimum temp \n",
    "- date of maximum temp \n",
    "- mean temp for each file. \n",
    "\n",
    "This data should all be appended for a dictionary within a for loop:    \n",
    "Key should be the file name without the path or .txt extension  \n",
    "Values should be (minTemp,maxTemp,minDate,maxDate,meanTemp)\n",
    "\n",
    "I recommend making this work for one file first, then putting the rest in a for loop to do the rest.  \n",
    "\n",
    "Below is an example of how to read in one file\n",
    "\n",
    "*hint:* do not read date in as date object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9/29/2013</td>\n",
       "      <td>8:00:00 AM</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9/29/2013</td>\n",
       "      <td>8:35:00 AM</td>\n",
       "      <td>34.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/29/2013</td>\n",
       "      <td>9:10:00 AM</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/29/2013</td>\n",
       "      <td>9:45:00 AM</td>\n",
       "      <td>35.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9/29/2013</td>\n",
       "      <td>10:20:00 AM</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date         Time  Temp\n",
       "0  9/29/2013   8:00:00 AM  34.7\n",
       "1  9/29/2013   8:35:00 AM  34.8\n",
       "2  9/29/2013   9:10:00 AM  35.0\n",
       "3  9/29/2013   9:45:00 AM  35.5\n",
       "4  9/29/2013  10:20:00 AM  37.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### set up data frame as you read in each file\n",
    "infile = pd.read_csv(logfiles[0],sep='\\t',engine='python')\n",
    "infile.columns = ['Index','Date','Time','Temp','Type']\n",
    "infile = infile[['Date','Time','Temp']]\n",
    "infile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do everything in steps, make sure it works. Calculate summaries with this one infile:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`minTemp =   \n",
    "maxTemp =  \n",
    "minDate = infile['Date'][infile['Temp'] == infile['Temp'].min()].unique()[0] #use this for minDate,maxDate  \n",
    "maxDate =   \n",
    "meanTemp =   `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you started, I suggest writing some dummy code in plain words to help outline your for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfiles_dict = {}  \n",
    "for f in logfiles:  \n",
    "    #do something   \n",
    "    #do not read date in as date object  \n",
    "    #do more something   \n",
    "    #do other stuff  \n",
    "    #make print statements EVERYWHERE  \n",
    "    #append to dict  \n",
    "    #blahbahblah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do the real code below here. You don't need to turnin your thoughts. Just put it in there as a help reminder. Most people all still do this, no matter how advanced they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once you have created a DataFrame with all the logfiles, print the head and save it to an outfile using pd.to_csv() as logfiles_df.csv** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
